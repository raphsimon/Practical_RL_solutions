{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr_aKWMGEmh-"
   },
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a __PyTorch__ neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oaMu65ONEmh_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: bash: command not found\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "avILCRKkEpaX"
   },
   "outputs": [],
   "source": [
    "# !pip install gymnasium[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K_SRk2ASEmh_"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x2YvkgprEmh_"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sI8W19CwEmh_"
   },
   "source": [
    "# Approximate Q-learning: building the network\n",
    "\n",
    "To train a neural network policy one must have a neural network policy. Let's build it.\n",
    "\n",
    "\n",
    "Since we're working with a pre-extracted features (cart positions, angles and velocities), we don't need a complicated network yet. In fact, let's build something like this for starters:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/yet_another_week/_resource/qlearning_scheme.png)\n",
    "\n",
    "For your first run, please only use linear layers (`nn.Linear`) and activations. Stuff like batch normalization or dropout may ruin everything if used haphazardly.\n",
    "\n",
    "Also please avoid using nonlinearities like sigmoid & tanh: since agent's observations are not normalized, sigmoids might be saturated at initialization. Instead, use non-saturating nonlinearities like ReLU.\n",
    "\n",
    "Ideally you should start small with maybe 1-2 hidden layers with < 200 neurons and then increase network size if agent doesn't beat the target score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YdWXv8WJEmiA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "y2-PcaIQEmiA"
   },
   "outputs": [],
   "source": [
    "network = nn.Sequential()\n",
    "\n",
    "network.add_module('layer1', nn.Linear(state_dim[0], 128))\n",
    "network.add_module('relu1', nn.ReLU())\n",
    "network.add_module('layer2', nn.Linear(128, 128))\n",
    "network.add_module('relu2', nn.ReLU())\n",
    "network.add_module('lawer3', nn.Linear(128 , n_actions))\n",
    "\n",
    "# hint: use state_dim[0] as input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "8xuWPGriEmiA"
   },
   "outputs": [],
   "source": [
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    state = torch.tensor(state[None], dtype=torch.float32)\n",
    "    q_values = network(state).detach().numpy()\n",
    "\n",
    "    if np.random.rand() > epsilon:\n",
    "        action = np.argmax(q_values)\n",
    "    else:\n",
    "        action = np.random.choice(np.arange(q_values.size))\n",
    "\n",
    "    return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "wroEfSRNEmiA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0.0 tests passed\n",
      "e=0.1 tests passed\n",
      "e=0.5 tests passed\n",
      "e=1.0 tests passed\n"
     ]
    }
   ],
   "source": [
    "s, _ = env.reset()\n",
    "assert tuple(network(torch.tensor([s]*3, dtype=torch.float32)).size()) == (\n",
    "    3, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert isinstance(list(network.modules(\n",
    "))[-1], nn.Linear), \"please make sure you predict q-values without nonlinearity (ignore if you know what you're doing)\"\n",
    "assert isinstance(get_action(s), int), \"get_action(s) must return int, not %s. try int(action)\" % (type(get_action(s)))\n",
    "\n",
    "# test epsilon-greedy exploration\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount(\n",
    "        [get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    assert abs(state_frequencies[best_action] -\n",
    "               10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_frequencies[other_action] -\n",
    "                       10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f tests passed' % eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f88ovLBQEmiA"
   },
   "source": [
    "### Q-learning via gradient descent\n",
    "\n",
    "We shall now train our agent's Q-function by minimizing the TD loss:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
    "\n",
    "\n",
    "Where\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "The tricky part is with  $Q_{-}(s',a')$. From an engineering standpoint, it's the same as $Q_{\\theta}$ - the output of your neural network policy. However, when doing gradient descent, __we won't propagate gradients through it__ to make training more stable (see lectures).\n",
    "\n",
    "To do so, we shall use `x.detach()` function which basically says \"consider this thing constant when doing backprop\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "bOIpO142EmiB"
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "    states = torch.tensor(states, dtype=torch.float32)                # shape: [batch_size, state_size]\n",
    "    actions = torch.tensor(actions, dtype=torch.long)                 # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)              # shape: [batch_size]\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)      # shape: [batch_size, state_size]\n",
    "    is_done = torch.tensor(is_done, dtype=torch.uint8)                # shape: [batch_size]\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = network(states)                               # shape: [batch_size, n_actions]\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[                # shape: [batch_size]\n",
    "      range(states.shape[0]), actions\n",
    "    ]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = network(next_states)\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values, _ = torch.max(predicted_next_qvalues, 1)\n",
    "    assert next_state_values.dtype == torch.float32\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # We use gamme * next_state_values, because next_state_values is the max of our predicted Q-values\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values\n",
    "\n",
    "    # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    target_qvalues_for_actions = torch.where(\n",
    "        is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "lKi6AK3DEmiB"
   },
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "s, _ = env.reset()\n",
    "a = env.action_space.sample()\n",
    "next_s, r, terminated, _, _ = env.step(a)\n",
    "loss = compute_td_loss([s], [a], [r], [next_s], [terminated], check_shapes=True)\n",
    "loss.backward()\n",
    "\n",
    "assert len(loss.size()) == 0, \"you must return scalar loss - mean over batch\"\n",
    "assert np.any(next(network.parameters()).grad.detach().numpy() !=\n",
    "              0), \"loss must be differentiable w.r.t. network weights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgL6G5lFEmiB"
   },
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "zsHb_fjjEmiB"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(network.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "nJ_-xtsjEmiB"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s, _ = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            compute_td_loss([s], [a], [r], [next_s], [terminated]).backward()\n",
    "            opt.step()\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "40mKYuVIEmiB"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "EXy8ij00EmiB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 13.610\tepsilon = 0.500\n",
      "epoch #1\tmean reward = 13.790\tepsilon = 0.495\n",
      "epoch #2\tmean reward = 14.180\tepsilon = 0.490\n",
      "epoch #3\tmean reward = 15.120\tepsilon = 0.485\n",
      "epoch #4\tmean reward = 16.140\tepsilon = 0.480\n",
      "epoch #5\tmean reward = 32.190\tepsilon = 0.475\n",
      "epoch #6\tmean reward = 38.060\tepsilon = 0.471\n",
      "epoch #7\tmean reward = 45.230\tepsilon = 0.466\n",
      "epoch #8\tmean reward = 54.410\tepsilon = 0.461\n",
      "epoch #9\tmean reward = 66.820\tepsilon = 0.457\n",
      "epoch #10\tmean reward = 93.010\tepsilon = 0.452\n",
      "epoch #11\tmean reward = 126.100\tepsilon = 0.448\n",
      "epoch #12\tmean reward = 128.870\tepsilon = 0.443\n",
      "epoch #13\tmean reward = 151.100\tepsilon = 0.439\n",
      "epoch #14\tmean reward = 175.960\tepsilon = 0.434\n",
      "epoch #15\tmean reward = 172.600\tepsilon = 0.430\n",
      "epoch #16\tmean reward = 163.330\tepsilon = 0.426\n",
      "epoch #17\tmean reward = 167.850\tepsilon = 0.421\n",
      "epoch #18\tmean reward = 200.540\tepsilon = 0.417\n",
      "epoch #19\tmean reward = 202.820\tepsilon = 0.413\n",
      "epoch #20\tmean reward = 309.820\tepsilon = 0.409\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session_rewards = [generate_session(env, epsilon=epsilon, train=True) for _ in range(100)]\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "\n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "\n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "We started off with a very simple architecture just using one input layer, an activation function (ReLU), and then one output layer with just 16 neurons but this was too simple, even after 120 epochs we were still hovering around 15 mean reward. Upon making network more complicated, as it is now, the policy was improved and we could reach a higher mean reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJPoF9XtEmiB"
   },
   "source": [
    "### How to interpret results\n",
    "\n",
    "\n",
    "Welcome to the f.. world of deep f...n reinforcement learning. Don't expect agent's reward to smoothly go up. Hope for it to go increase eventually. If it deems you worthy.\n",
    "\n",
    "Seriously though,\n",
    "* __ mean reward__ is the average reward per game. For a correct implementation it may stay low for some 10 epochs, then start growing while oscilating insanely and converges by ~50-100 steps depending on the network architecture.\n",
    "* If it never reaches target score by the end of for loop, try increasing the number of hidden neurons or look at the epsilon.\n",
    "* __ epsilon__ - agent's willingness to explore. If you see that agent's already at < 0.01 epsilon before it's is at least 200, just reset it back to 0.1 - 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhKiN-qOEmiB"
   },
   "source": [
    "### Record videos\n",
    "\n",
    "As usual, we now use `gymnasium.wrappers.RecordVideo` to record a video of our agent playing the game. Unlike our previous attempts with state binarization, this time we expect our agent to act ~~(or fail)~~ more smoothly since there's no more binarization error at play.\n",
    "\n",
    "As you already did with tabular q-learning, we set epsilon=0 for final evaluation to prevent agent from exploring himself to death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "2yqPkj6HEmiB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsimon/miniconda3/envs/practical_rl/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:87: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/rsimon/notebooks/Practical_RL_solutions/week04_approx_rl/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/rsimon/notebooks/Practical_RL_solutions/week04_approx_rl/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/rsimon/notebooks/Practical_RL_solutions/week04_approx_rl/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 1/202 [00:00<00:02, 75.75it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/rsimon/notebooks/Practical_RL_solutions/week04_approx_rl/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/rsimon/notebooks/Practical_RL_solutions/week04_approx_rl/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 0/202 [00:00<?, ?it/s, now=None]\u001b[A"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 32] Broken pipe\n\nMoviePy error: FFMPEG encountered the following error while writing file /home/rsimon/notebooks/Practical_RL_solutions/week04_approx_rl/videos/rl-video-episode-0.mp4:\n\n b\"Unrecognized option 'preset'.\\nError splitting the argument list: Option not found\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:136\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.write_frame\u001b[0;34m(self, img_array)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PY3:\n\u001b[0;32m--> 136\u001b[0m    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite(img_array\u001b[38;5;241m.\u001b[39mtobytes())\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m record_env, RecordVideo(\n\u001b[1;32m      6\u001b[0m     record_env, video_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m env_monitor:\n\u001b[0;32m----> 8\u001b[0m     sessions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m         generate_session(env_monitor, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     10\u001b[0m     ]\n",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m record_env, RecordVideo(\n\u001b[1;32m      6\u001b[0m     record_env, video_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m env_monitor:\n\u001b[1;32m      8\u001b[0m     sessions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 9\u001b[0m         generate_session(env_monitor, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     10\u001b[0m     ]\n",
      "Cell \u001b[0;32mIn[50], line 8\u001b[0m, in \u001b[0;36mgenerate_session\u001b[0;34m(env, t_max, epsilon, train)\u001b[0m\n\u001b[1;32m      7\u001b[0m a \u001b[38;5;241m=\u001b[39m get_action(s, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[0;32m----> 8\u001b[0m next_s, r, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:180\u001b[0m, in \u001b[0;36mRecordVideo.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminateds \u001b[38;5;129;01mor\u001b[39;00m truncateds:\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose_video_recorder()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m terminateds[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m truncateds[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:193\u001b[0m, in \u001b[0;36mRecordVideo.close_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:161\u001b[0m, in \u001b[0;36mVideoRecorder.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m     moviepy_logger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_logger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 161\u001b[0m     clip\u001b[38;5;241m.\u001b[39mwrite_videofile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, logger\u001b[38;5;241m=\u001b[39mmoviepy_logger)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# No frames captured. Set metadata.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/decorators.py:125\u001b[0m, in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    122\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    123\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m k\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39mnew_a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kw)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/decorators.py:22\u001b[0m, in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     21\u001b[0m     clip \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mto_RGB()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/video/VideoClip.py:300\u001b[0m, in \u001b[0;36mVideoClip.write_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mwrite_audiofile(audiofile, audio_fps,\n\u001b[1;32m    294\u001b[0m                                audio_nbytes, audio_bufsize,\n\u001b[1;32m    295\u001b[0m                                audio_codec, bitrate\u001b[38;5;241m=\u001b[39maudio_bitrate,\n\u001b[1;32m    296\u001b[0m                                write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile,\n\u001b[1;32m    297\u001b[0m                                verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    298\u001b[0m                                logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[0;32m--> 300\u001b[0m ffmpeg_write_video(\u001b[38;5;28mself\u001b[39m, filename, fps, codec,\n\u001b[1;32m    301\u001b[0m                    bitrate\u001b[38;5;241m=\u001b[39mbitrate,\n\u001b[1;32m    302\u001b[0m                    preset\u001b[38;5;241m=\u001b[39mpreset,\n\u001b[1;32m    303\u001b[0m                    write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile,\n\u001b[1;32m    304\u001b[0m                    audiofile\u001b[38;5;241m=\u001b[39maudiofile,\n\u001b[1;32m    305\u001b[0m                    verbose\u001b[38;5;241m=\u001b[39mverbose, threads\u001b[38;5;241m=\u001b[39mthreads,\n\u001b[1;32m    306\u001b[0m                    ffmpeg_params\u001b[38;5;241m=\u001b[39mffmpeg_params,\n\u001b[1;32m    307\u001b[0m                    logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_temp \u001b[38;5;129;01mand\u001b[39;00m make_audio:\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:228\u001b[0m, in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    226\u001b[0m             frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdstack([frame,mask])\n\u001b[0;32m--> 228\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_frame(frame)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m write_logfile:\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:180\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.write_frame\u001b[0;34m(self, img_array)\u001b[0m\n\u001b[1;32m    176\u001b[0m     error \u001b[38;5;241m=\u001b[39m error \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe video export failed because the codec \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor file extension you provided is not a video\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(error)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 32] Broken pipe\n\nMoviePy error: FFMPEG encountered the following error while writing file /home/rsimon/notebooks/Practical_RL_solutions/week04_approx_rl/videos/rl-video-episode-0.mp4:\n\n b\"Unrecognized option 'preset'.\\nError splitting the argument list: Option not found\\n\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:136\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.write_frame\u001b[0;34m(self, img_array)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PY3:\n\u001b[0;32m--> 136\u001b[0m    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite(img_array\u001b[38;5;241m.\u001b[39mtobytes())\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Record sessions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecordVideo\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m record_env, RecordVideo(\n\u001b[1;32m      6\u001b[0m     record_env, video_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m env_monitor:\n\u001b[1;32m      8\u001b[0m     sessions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m         generate_session(env_monitor, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     10\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/gymnasium/core.py:232\u001b[0m, in \u001b[0;36mEnv.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any):\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Support with-statement for the environment and closes the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# propagate exception\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:217\u001b[0m, in \u001b[0;36mRecordVideo.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Closes the wrapper then the video recorder.\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose_video_recorder()\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:193\u001b[0m, in \u001b[0;36mRecordVideo.close_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:161\u001b[0m, in \u001b[0;36mVideoRecorder.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m     clip \u001b[38;5;241m=\u001b[39m ImageSequenceClip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes_per_sec)\n\u001b[1;32m    160\u001b[0m     moviepy_logger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_logger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 161\u001b[0m     clip\u001b[38;5;241m.\u001b[39mwrite_videofile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, logger\u001b[38;5;241m=\u001b[39mmoviepy_logger)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# No frames captured. Set metadata.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/decorators.py:125\u001b[0m, in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    120\u001b[0m new_a \u001b[38;5;241m=\u001b[39m [fun(arg) \u001b[38;5;28;01mif\u001b[39;00m (name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[1;32m    121\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, names)]\n\u001b[1;32m    122\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    123\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m k\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39mnew_a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kw)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/decorators.py:22\u001b[0m, in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip\u001b[38;5;241m.\u001b[39mismask:\n\u001b[1;32m     21\u001b[0m     clip \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mto_RGB()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/video/VideoClip.py:300\u001b[0m, in \u001b[0;36mVideoClip.write_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_audio:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mwrite_audiofile(audiofile, audio_fps,\n\u001b[1;32m    294\u001b[0m                                audio_nbytes, audio_bufsize,\n\u001b[1;32m    295\u001b[0m                                audio_codec, bitrate\u001b[38;5;241m=\u001b[39maudio_bitrate,\n\u001b[1;32m    296\u001b[0m                                write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile,\n\u001b[1;32m    297\u001b[0m                                verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    298\u001b[0m                                logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[0;32m--> 300\u001b[0m ffmpeg_write_video(\u001b[38;5;28mself\u001b[39m, filename, fps, codec,\n\u001b[1;32m    301\u001b[0m                    bitrate\u001b[38;5;241m=\u001b[39mbitrate,\n\u001b[1;32m    302\u001b[0m                    preset\u001b[38;5;241m=\u001b[39mpreset,\n\u001b[1;32m    303\u001b[0m                    write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile,\n\u001b[1;32m    304\u001b[0m                    audiofile\u001b[38;5;241m=\u001b[39maudiofile,\n\u001b[1;32m    305\u001b[0m                    verbose\u001b[38;5;241m=\u001b[39mverbose, threads\u001b[38;5;241m=\u001b[39mthreads,\n\u001b[1;32m    306\u001b[0m                    ffmpeg_params\u001b[38;5;241m=\u001b[39mffmpeg_params,\n\u001b[1;32m    307\u001b[0m                    logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_temp \u001b[38;5;129;01mand\u001b[39;00m make_audio:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(audiofile):\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:228\u001b[0m, in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    225\u001b[0m                 mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    226\u001b[0m             frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdstack([frame,mask])\n\u001b[0;32m--> 228\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_frame(frame)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m write_logfile:\n\u001b[1;32m    231\u001b[0m     logfile\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/practical_rl/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:180\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.write_frame\u001b[0;34m(self, img_array)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid encoder type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ffmpeg_error:\n\u001b[1;32m    176\u001b[0m     error \u001b[38;5;241m=\u001b[39m error \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe video export failed because the codec \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor file extension you provided is not a video\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(error)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 32] Broken pipe\n\nMoviePy error: FFMPEG encountered the following error while writing file /home/rsimon/notebooks/Practical_RL_solutions/week04_approx_rl/videos/rl-video-episode-0.mp4:\n\n b\"Unrecognized option 'preset'.\\nError splitting the argument list: Option not found\\n\""
     ]
    }
   ],
   "source": [
    "# Record sessions\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "with gym.make(\"CartPole-v0\", render_mode=\"rgb_array\") as record_env, RecordVideo(\n",
    "    record_env, video_folder=\"videos\"\n",
    ") as env_monitor:\n",
    "    sessions = [\n",
    "        generate_session(env_monitor, epsilon=0, train=False) for _ in range(100)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afqi2qomEmiC"
   },
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open('rb') as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Practical_RL",
   "language": "python",
   "name": "practical_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
